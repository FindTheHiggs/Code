{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylorentz import Momentum4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_Files():\n",
    "    \n",
    "    #FORMAT:\n",
    "    # mcWeight, SumWeights, sf_Pileup, sf_Photon, sf_Btag, sf_Trigger, \n",
    "    # Gam1(pt,eta,phi,E, trigM, Tight, ptCone, etaCone),Gam2(...), lep_n, trigP, jet_n, jet1(...,Btag),...\n",
    "\n",
    "    #Input files to read, scale factor files to write, and data files to write\n",
    "    in_files = ['ggHout.csv', 'dataAout.csv', 'dataBout.csv', 'dataCout.csv','dataDout.csv', 'yyjj_p1.csv']\n",
    "    sf_files = ['ggH_sf.csv', 'dataA_sf.csv', 'dataB_sf.csv', 'dataC_sf.csv', 'dataD_sf.csv']\n",
    "    data_files = ['ggH.csv','dataA.csv','dataB.csv','dataC.csv','dataD.csv']\n",
    "    \n",
    "    #Due to ragged CSV files, must find the file with the longest row\n",
    "    #Then column names can be generated and passed to pandas\n",
    "    #when opening the file\n",
    "    max_lens = []\n",
    "    for i in range(len(in_files)):\n",
    "\n",
    "        max_row = len(max(open('./../CSVfiles/{}'.format(in_files[i]), 'r'),key=len).split(',')) -1\n",
    "        max_lens.append(max_row)\n",
    "\n",
    "    biggest_row = max(max_lens) #This is 100, so make col names 100 long\n",
    "    col_names = ['mcweight', 'sumweights', 'sf_pileup', 'sf_photon', 'sf_btag', 'sf_trigger', 'g1_pt','g1_eta',\n",
    "                 'g1_phi','g1_E','g1_trigm','g1_tight','g1_ptcone','g1_etcone','g2_pt','g2_eta',\n",
    "                 'g2_phi','g2_E','g2_trigm','g2_tight','g2_ptcone','g2_etcone', 'lep_n', 'trigP', 'jet_n',\n",
    "                 'j1_pt','j1_eta','j1_phi','j1_E','j1_MV2C10','j2_pt','j2_eta','j2_phi','j2_E','j2_MV2C10',\n",
    "                 'j3_pt','j3_eta','j3_phi','j3_E','j3_MV2C10','j4_pt','j4_eta','j4_phi','j4_E','j4_MV2C10',\n",
    "                 'j5_pt','j5_eta','j5_phi','j5_E','j5_MV2C10','j6_pt','j6_eta','j6_phi','j6_E','j6_MV2C10',\n",
    "                 'j7_pt','j7_eta','j7_phi','j7_E','j7_MV2C10','j8_pt','j8_eta','j8_phi','j8_E','j8_MV2C10',\n",
    "                 'j9_pt','j9_eta','j9_phi','j9_E','j9_MV2C10','j10_pt','j10_eta','j10_phi','j10_E','j10_MV2C10',\n",
    "                 'j11_pt','j11_eta','j11_phi','j11_E','j11_MV2C10','j12_pt','j12_eta','j12_phi','j12_E','j12_MV2C10',\n",
    "                 'j13_pt','j13_eta','j13_phi','j13_E','j13_MV2C10','j14_pt','j14_eta','j14_phi','j14_E','j14_MV2C10',\n",
    "                 'j15_pt','j15_eta','j15_phi','j15_E','j15_MV2C10']\n",
    "    \n",
    "    #Parse through each file, process it, then write to the new files\n",
    "    for i in range(len(in_files)):\n",
    "        \n",
    "        #For all but the MC background file (becuase this file has a unique structure)\n",
    "        if (in_files[i] != 'yyjj_p1.csv'):\n",
    "\n",
    "            #Read CSV\n",
    "            df = pd.read_csv('./../CSVfiles/{}'.format(in_files[i]), names = col_names)\n",
    "\n",
    "            #Replace NaN values with integer\n",
    "            df.fillna(-10**6, inplace = True)\n",
    "\n",
    "            #Filter out entries with > 0 leptons, then get rid of the column\n",
    "            indexes = df.loc[df['lep_n'] > 0].index\n",
    "            df.drop(indexes,inplace = True)\n",
    "            df.drop(['lep_n', 'jet_n',\n",
    "                 'j3_pt','j3_eta','j3_phi','j3_E','j3_MV2C10','j4_pt','j4_eta','j4_phi','j4_E','j4_MV2C10',\n",
    "                 'j5_pt','j5_eta','j5_phi','j5_E','j5_MV2C10','j6_pt','j6_eta','j6_phi','j6_E','j6_MV2C10',\n",
    "                 'j7_pt','j7_eta','j7_phi','j7_E','j7_MV2C10','j8_pt','j8_eta','j8_phi','j8_E','j8_MV2C10',\n",
    "                 'j9_pt','j9_eta','j9_phi','j9_E','j9_MV2C10','j10_pt','j10_eta','j10_phi','j10_E','j10_MV2C10',\n",
    "                 'j11_pt','j11_eta','j11_phi','j11_E','j11_MV2C10','j12_pt','j12_eta','j12_phi','j12_E','j12_MV2C10',\n",
    "                 'j13_pt','j13_eta','j13_phi','j13_E','j13_MV2C10','j14_pt','j14_eta','j14_phi','j14_E','j14_MV2C10',\n",
    "                 'j15_pt','j15_eta','j15_phi','j15_E','j15_MV2C10'], axis=1, inplace = True)\n",
    "                \n",
    "            #Use PyLorentz to calculate the quantities associated with the\n",
    "            #parent of the two photons\n",
    "            inv_masses, trans_momenta, energies, etas, phis = parent_Quantities(df[['g1_pt', 'g1_eta', 'g1_phi', 'g1_E',\n",
    "                                                                                    'g2_pt', 'g2_eta', 'g2_phi', 'g2_E']].values)\n",
    "\n",
    "\n",
    "            #Calculate photon separation angle (eq: sqrt(eta1-eta2)^2+(phi1-phi2)^2))\n",
    "\n",
    "            del_r = np.sqrt((df['g1_eta']-df['g2_eta'])**2 + (df['g1_phi']-df['g2_phi'])**2)\n",
    "\n",
    "            #Add the new columns to the dataframe\n",
    "            df.insert(loc=0, column='photon_sep', value=del_r)\n",
    "            df.insert(loc=0, column='p_E', value=energies)\n",
    "            df.insert(loc=0, column='p_phi', value=phis)\n",
    "            df.insert(loc=0, column='p_eta', value=etas)\n",
    "            df.insert(loc=0, column='p_pt', value=trans_momenta)\n",
    "            df.insert(loc=0, column='p_mass', value=inv_masses)\n",
    "            \n",
    "            #Can remove rows of data set if at least 1 photon: is not tight, \n",
    "            #or has ptCone/pt > 0.05, or has etaCone/eta > 0.065, or mass outside 90 - 170 GeV range\n",
    "            pt1Ratio = df['g1_ptcone']/df['g1_pt']\n",
    "            pt2Ratio = df['g2_ptcone']/df['g2_pt']\n",
    "            et1Ratio = df['g1_etcone']/df['g1_pt']\n",
    "            et2Ratio = df['g2_etcone']/df['g2_pt']\n",
    "\n",
    "            remove_tight = df[(df['g1_tight'] == 0) | (df['g2_tight'] == 0)].index\n",
    "            print('Tight condition: %.2f'%(len(remove_tight)/len(df)))\n",
    "            remove_ptratio = df[(pt1Ratio > 0.05) | (pt2Ratio > 0.05)].index\n",
    "            print('Ptratio condition: %.2f'%(len(remove_ptratio)/len(df)))\n",
    "            remove_etratio = df[(et1Ratio > 0.065) | (et2Ratio > 0.065)].index\n",
    "            print('Etratio condition: %.2f'%(len(remove_etratio)/len(df)))\n",
    "            remove_mass = df[(df['p_mass'] < 105) | (df['p_mass'] > 165)].index\n",
    "            print('Mass condition: %.2f'%(len(remove_mass)/len(df)))\n",
    "            remove_mass_ratio = df[(df['g1_pt']/df['p_mass']) > 0.35 | (df['g2_pt']/df['p_mass']) > 0.25].index\n",
    "            print('Pt/Mass condition: %.2f'%(len(remove_mass)/len(df)))\n",
    "            \n",
    "            del remove_tight,remove_ptratio,remove_etratio,remove_mass\n",
    "            \n",
    "            inds_to_remove = df[((df['g1_tight'] == 0) | (df['g2_tight'] == 0)) | ((pt1Ratio > 0.05) \n",
    "                            | (pt2Ratio > 0.05)) | ((et1Ratio > 0.065) | (et2Ratio > 0.065)) \n",
    "                            | ((df['p_mass'] < 105) | (df['p_mass'] > 165))\n",
    "                            | ((df['g1_pt']/df['p_mass']) > 0.35 | (df['g2_pt']/df['p_mass']) > 0.25)].index\n",
    "            print('Overall: %.2f'%(len(inds_to_remove)/len(df)))\n",
    "            df.drop(inds_to_remove, axis = 0, inplace = True)\n",
    "            \n",
    "            #Make masses in GeV\n",
    "            df['p_mass'] = df['p_mass']/1000\n",
    "            \n",
    "            #Generate a label column for ggH data\n",
    "            if (in_files[i] == 'ggHout.csv'):\n",
    "                labels = np.ones(len(df))\n",
    "                df.insert(loc=0, column='label', value=labels)\n",
    "\n",
    "            #Isolate scale factor data, then remove this to isolate particle data\n",
    "            sf_df = df[['mcweight', 'sumweights', 'sf_pileup', 'sf_photon', 'sf_btag', 'sf_trigger']]\n",
    "            df.drop(['mcweight', 'sumweights', 'sf_pileup', 'sf_photon', 'sf_btag', 'sf_trigger'], axis=1, inplace = True)\n",
    "\n",
    "            #Write scale factor data\n",
    "            sf_df.to_csv(\"./../CSVfiles/{}\".format(sf_files[i]), index = False, header = True)\n",
    "            #Write all other data\n",
    "            df.to_csv(\"./../CSVfiles/{}\".format(data_files[i]), index = False, header = True)\n",
    "                \n",
    "            print('Done {}'.format(in_files[i]))\n",
    "            \n",
    "        elif (in_files[i] == 'yyjj_p1.csv'): \n",
    "            gen_ggjj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need a different function for the ggjj dataset, as it has a differnet layout\n",
    "def gen_ggjj():\n",
    "    \n",
    "    #FORMAT:\n",
    "    # m_weight,y1_Pt ,y1_Eta ,y1_Phi , y1_E, m_y1_is_tight ,m_y1_ptcone20 ,\n",
    "    #m_y1_topoetcone20,m_y1_is_isolated, y2.Pt ,y2.Eta ,y2.Phi , y2.E,\n",
    "    #m_y2_is_tight ,m_y2_ptcone20 ,m_y2_topoetcone20, m_y2_is_isolated, \n",
    "    #myy.M, njet, jet.pt ,jet.eta  jet.phi ,jet.e ,is_btag_70\n",
    "    \n",
    "    #THROW OUT:\n",
    "    #m_y1_ptcone20, m_y1_topoetcone20, m_y1_is_isolated, _is_btag_70 \n",
    "    \n",
    "    col_names = ['m_weight', 'g1_pt','g1_eta',\n",
    "                 'g1_phi','g1_E','g1_tight','g1_ptcone','g1_etcone','m_y1_is_isolated','g2_pt','g2_eta',\n",
    "                 'g2_phi','g2_E','g2_tight','g2_ptcone','g2_etcone','m_y2_is_isolated', 'p_mass', 'jet_n',\n",
    "                 'j1_pt','j1_eta','j1_phi','j1_E','j1_MV2C10','j2_pt','j2_eta','j2_phi','j2_E','j2_MV2C10',\n",
    "                 'j3_pt','j3_eta','j3_phi','j3_E','j3_MV2C10','j4_pt','j4_eta','j4_phi','j4_E','j4_MV2C10',\n",
    "                 'j5_pt','j5_eta','j5_phi','j5_E','j5_MV2C10','j6_pt','j6_eta','j6_phi','j6_E','j6_MV2C10',\n",
    "                 'j7_pt','j7_eta','j7_phi','j7_E','j7_MV2C10','j8_pt','j8_eta','j8_phi','j8_E','j8_MV2C10',\n",
    "                 'j9_pt','j9_eta','j9_phi','j9_E','j9_MV2C10','j10_pt','j10_eta','j10_phi','j10_E','j10_MV2C10',\n",
    "                 'j11_pt','j11_eta','j11_phi','j11_E','j11_MV2C10','j12_pt','j12_eta','j12_phi','j12_E','j12_MV2C10',\n",
    "                 'j13_pt','j13_eta','j13_phi','j13_E','j13_MV2C10','j14_pt','j14_eta','j14_phi','j14_E','j14_MV2C10',\n",
    "                 'j15_pt','j15_eta','j15_phi','j15_E','j15_MV2C10']\n",
    "\n",
    "    #Read CSV\n",
    "    df = pd.read_csv('./../CSVfiles/yyjj_p1.csv', names = col_names)\n",
    "\n",
    "    #Replace NaN values with integer\n",
    "    df.fillna(-10**6, inplace = True)\n",
    "        \n",
    "    #Get rid of columns with no dataABCD equivalent\n",
    "    df.drop(['m_y1_is_isolated', 'm_y2_is_isolated', 'jet_n',\n",
    "                 'j3_pt','j3_eta','j3_phi','j3_E','j3_MV2C10','j4_pt','j4_eta','j4_phi','j4_E','j4_MV2C10',\n",
    "                 'j5_pt','j5_eta','j5_phi','j5_E','j5_MV2C10','j6_pt','j6_eta','j6_phi','j6_E','j6_MV2C10',\n",
    "                 'j7_pt','j7_eta','j7_phi','j7_E','j7_MV2C10','j8_pt','j8_eta','j8_phi','j8_E','j8_MV2C10',\n",
    "                 'j9_pt','j9_eta','j9_phi','j9_E','j9_MV2C10','j10_pt','j10_eta','j10_phi','j10_E','j10_MV2C10',\n",
    "                 'j11_pt','j11_eta','j11_phi','j11_E','j11_MV2C10','j12_pt','j12_eta','j12_phi','j12_E','j12_MV2C10',\n",
    "                 'j13_pt','j13_eta','j13_phi','j13_E','j13_MV2C10','j14_pt','j14_eta','j14_phi','j14_E','j14_MV2C10',\n",
    "                 'j15_pt','j15_eta','j15_phi','j15_E','j15_MV2C10'], axis = 1, inplace = True)\n",
    "    \n",
    "    #Can remove rows of data set if at least 1 photon: is not tight, \n",
    "    #or has ptCone/pt > 0.05, or has etaCone/eta > 0.065, or mass outside 105 - 165 GeV range\n",
    "\n",
    "    pt1Ratio = df['g1_ptcone']/df['g1_pt']\n",
    "    pt2Ratio = df['g2_ptcone']/df['g2_pt']\n",
    "    et1Ratio = df['g1_etcone']/df['g1_pt']\n",
    "    et2Ratio = df['g2_etcone']/df['g2_pt']\n",
    "\n",
    "    remove_tight = df[(df['g1_tight'] == 0) | (df['g2_tight'] == 0)].index\n",
    "    print('Tight condition: %.2f'%(len(remove_tight)/len(df)))\n",
    "    remove_ptratio = df[(pt1Ratio > 0.05) | (pt2Ratio > 0.05)].index\n",
    "    print('Ptratio condition: %.2f'%(len(remove_ptratio)/len(df)))\n",
    "    remove_etratio = df[(et1Ratio > 0.065) | (et2Ratio > 0.065)].index\n",
    "    print('Etratio condition: %.2f'%(len(remove_etratio)/len(df)))\n",
    "    remove_mass = df[(df['p_mass'] < 105) | (df['p_mass'] > 165)].index\n",
    "    print('Mass condition: %.2f'%(len(remove_mass)/len(df)))\n",
    "    remove_mass_ratio = df[(df['g1_pt']/df['p_mass']) > 0.35 | (df['g2_pt']/df['p_mass']) > 0.25].index\n",
    "    print('Pt/Mass condition: %.2f'%(len(remove_mass)/len(df)))\n",
    "            \n",
    "    del remove_tight,remove_ptratio,remove_etratio,remove_mass\n",
    "            \n",
    "    inds_to_remove = df[((df['g1_tight'] == 0) | (df['g2_tight'] == 0)) | ((pt1Ratio > 0.05) \n",
    "                    | (pt2Ratio > 0.05)) | ((et1Ratio > 0.065) | (et2Ratio > 0.065)) \n",
    "                    | ((df['p_mass'] < 105) | (df['p_mass'] > 165))\n",
    "                    | ((df['g1_pt']/df['p_mass']) > 0.35 | (df['g2_pt']/df['p_mass']) > 0.25)].index\n",
    "    print('Overall: %.2f'%(len(inds_to_remove)/len(df)))\n",
    "    df.drop(inds_to_remove, axis = 0, inplace = True)\n",
    "    \n",
    "    #Masses in GeV\n",
    "    df['p_mass'] = df['p_mass']/1000\n",
    "    \n",
    "    #Calculate photon separation angle (eq: sqrt(eta1-eta2)^2+(phi1-phi2)^2))\n",
    "    del_r = np.sqrt((df['g1_eta']-df['g2_eta'])**2 + (df['g1_phi']-df['g2_phi'])**2)\n",
    "        \n",
    "    #This is a data file of background events, hence\n",
    "    #all labels will be 0\n",
    "    truths = np.zeros(len(df))\n",
    "    \n",
    "    #Use PyLorentz to calculate the quantities associated with the\n",
    "    #parent of the two photons\n",
    "    inv_masses, trans_momenta, energies, etas, phis = parent_Quantities(df[['g1_pt', 'g1_eta', 'g1_phi', 'g1_E',\n",
    "                                                                            'g2_pt', 'g2_eta', 'g2_phi', 'g2_E']].values)\n",
    "            \n",
    "    #Add the new columns to the dataframe\n",
    "    #To run ML, we will need the outputted csv to have the same number of columns \n",
    "    #as the ggH and dataABCD files, so fill the duds with NaN replacement\n",
    "    #***However, should drop such columns before training as it will\n",
    "    #allow the algorithm to cheat***\n",
    "    \n",
    "    df.insert(loc=0, column='photon_sep', value=del_r)\n",
    "    df.insert(loc=0, column='p_E', value=energies)\n",
    "    df.insert(loc=0, column='p_phi', value=phis)\n",
    "    df.insert(loc=0, column='p_eta', value=etas)\n",
    "    df.insert(loc=0, column='p_pt', value=trans_momenta)\n",
    "    df.insert(loc=0, column='label', value=truths)\n",
    "    df.insert(loc=11, column='g1_trigm', value=-1000000)\n",
    "    df.insert(loc=19, column='g2_trigm', value=-1000000)\n",
    "    df.insert(loc=23, column='trigP', value=-1000000)\n",
    "        \n",
    "    #Move mass column to start of dataframe (mass is currently the 24th column)\n",
    "    cols = df.columns.tolist()\n",
    "    cols = [cols[0]] + [cols[24]] + cols[1:24] + cols[25:]\n",
    "    df = df[cols]\n",
    "\n",
    "    #Isolate scale factor data, then remove this to isolate particle data\n",
    "    sf_df = df[['m_weight']]\n",
    "    df.drop(['m_weight'], axis=1, inplace = True)\n",
    "        \n",
    "    sf_df.to_csv('./../CSVfiles/sf_yyjj_p1.csv', index = False, header = True)\n",
    "    df.to_csv('./../CSVfiles/data_yyjj_p1.csv', index = False, header = True)\n",
    "        \n",
    "    print('Done ggjj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates truth vales upon a threshold cut\n",
    "def calc_Truths(y_pred, y_val, m, sf_val, threshold):\n",
    "\n",
    "    true_pos = []\n",
    "    false_pos = []\n",
    "    true_neg = []\n",
    "    false_neg = []\n",
    "\n",
    "    for i in range(len(y_val)):\n",
    "\n",
    "        if y_val[i] == 1 and y_pred[i] >= threshold:\n",
    "            true_pos.append([m[i], sf_val[i]])\n",
    "        elif y_val[i] == 0 and y_pred[i] >= threshold:\n",
    "            false_pos.append([m[i], sf_val[i]])\n",
    "        elif y_val[i] == 1 and y_pred[i] < threshold:\n",
    "            false_neg.append([m[i], sf_val[i]])\n",
    "        elif y_val[i] == 0 and y_pred[i] < threshold:\n",
    "            true_neg.append([m[i], sf_val[i]])\n",
    "\n",
    "    return true_pos, false_pos, true_neg, false_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sensitivity(true_pos, false_pos, sf_trim, SF_band):\n",
    "    \n",
    "    s, b, band_bkg = 0, 0, 0\n",
    "    for i in range(len(true_pos)):\n",
    "        if (true_pos[i][0] > 121) and (true_pos[i][0] < 129):\n",
    "            s += true_pos[i][1]\n",
    "    for i in range(len(false_pos)):\n",
    "        if (false_pos[i][0] > 121) and (false_pos[i][0] < 129):\n",
    "            b += false_pos[i][1]\n",
    "        elif (false_pos[i][0] <= 121) or (false_pos[i][0] >= 129):\n",
    "            band_bkg += false_pos[i][1]\n",
    "    \n",
    "    s = s*sf_trim[0]*(2.00419*(10**-3))\n",
    "    b = b*sf_trim[1]*SF_band\n",
    "    n = (s+b)\n",
    "    print('%.2f '%s,' %.2f '%b,' %.2f '%band_bkg)\n",
    "    sensitivity = np.sqrt(2*(n*np.log(n/b)+b-n))\n",
    "\n",
    "    return sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to turn a list into PyLorentz quantities\n",
    "def lorentzify(lst):\n",
    "\n",
    "    gamma_objects = []\n",
    "\n",
    "    #Separate each photon\n",
    "    each_gamma = np.split(lst, 2)\n",
    "\n",
    "    #Change each gamma into a PyLorentz object\n",
    "    for j in range(2):\n",
    "        gamma_objects.append(Momentum4.e_eta_phi_pt(each_gamma[j][3],each_gamma[j][1], each_gamma[j][2], each_gamma[j][0]))\n",
    "\n",
    "    return gamma_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use PyLorentz to calculate parent particle quantities\n",
    "def parent_Quantities(lst):\n",
    "\n",
    "    #Set memory placeholders for each list to avoid appends\n",
    "    inv_masses = np.zeros(len(lst))\n",
    "    trans_momenta = np.zeros(len(lst))\n",
    "    energies = np.zeros(len(lst))\n",
    "    etas = np.zeros(len(lst))\n",
    "    phis = np.zeros(len(lst))\n",
    "\n",
    "    for i in range(len(lst)):\n",
    "\n",
    "        #Turn list into PyLorentz objects\n",
    "        gammas = lorentzify(lst[i])\n",
    "        parent = gammas[0] + gammas[1]\n",
    "\n",
    "        #Calculate quantities\n",
    "        inv_masses[i] = parent.m\n",
    "        trans_momenta[i] = parent.p_t\n",
    "        energies[i] = parent.e\n",
    "        etas[i] = parent.eta\n",
    "        phis[i] = parent.phi\n",
    "    \n",
    "    return inv_masses, trans_momenta, energies, etas, phis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile (m, ys, labels=None, bins=np.linspace(100,170,50,endpoint=True), ax=None):\n",
    "    \n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    plt.figure(figsize=(12,6), dpi= 100)\n",
    "    # Check(s)\n",
    "    if isinstance(bins, int):\n",
    "        bins = np.linspace(m.min(), m.max(), bins + 1, endpoint=True)\n",
    "        pass\n",
    "\n",
    "    if not isinstance(ys, list):\n",
    "        ys = [ys]\n",
    "        pass\n",
    "\n",
    "    N = len(ys)\n",
    "    centres = bins[:-1] + 0.5 * np.diff(bins)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [None for _ in range(N)]\n",
    "    elif isinstance(labels, str):\n",
    "        labels = [labels]\n",
    "        pass\n",
    "\n",
    "    assert len(labels) == N, \"[profile] Number of observables ({}) and associated labels ({}) do not match.\".format(N, len(labels))\n",
    "\n",
    "    # Local background efficiency\n",
    "    profiles = {ix: list() for ix in range(N)}\n",
    "    means_NN  = list()\n",
    "    means_ANN = list()\n",
    "    for down, up in zip(bins[:-1], bins[1:]):\n",
    "        msk = (m >= down) & (m < up)\n",
    "        for ix, y in enumerate(ys):\n",
    "            profiles[ix].append(y[msk].mean())\n",
    "            pass\n",
    "        pass\n",
    "\n",
    "    # Ensure axes exist\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(6,5))\n",
    "        pass\n",
    "\n",
    "    # Plot profile(s)\n",
    "    for ix in range(N):\n",
    "        ax.hist(profiles[ix], bins=100, label=labels[ix])\n",
    "        pass\n",
    "\n",
    "    # Decorations\n",
    "    ax.set_xlabel('Mass [GeV]')\n",
    "    ax.set_ylabel('Average Value')\n",
    "    ax.set_ylim((0,1))\n",
    "    ax.set_xlim(bins[0], bins[-1])\n",
    "    ax.legend()\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
