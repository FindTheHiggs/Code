{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylorentz import Momentum4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_Files():\n",
    "\n",
    "    #Input files to read, scale factor files to write, data files to write, and files for background data from the ATLAS data\n",
    "    in_files = ['data15.csv', 'data16_p1.csv', 'data16_p2.csv', 'data16_p3.csv','ggF_atlas.csv','yyjj_p1.csv']\n",
    "    sf_files = ['data15_sf.csv', 'data16_p1_sf.csv', 'data16_p2_sf.csv', 'data16_p3_sf.csv','ggF_sf.csv','yyjj_sf.csv']\n",
    "    data_files = ['data15_data.csv','data16_p1_data.csv','data16_p2_data.csv','data16_p3_data.csv','ggF_data.csv','yyjj_data.csv']\n",
    "    data_bkgs = ['data15_bkg.csv','data16_p1_bkg.csv','data16_p2_bkg.csv','data16_p3_bkg.csv']\n",
    "    \n",
    "    #Enough column names to deal with ragged CSV file (some fields will be NaN)\n",
    "    col_names = ['mcweight', 'g1_pt','g1_eta',\n",
    "                 'g1_phi','g1_E','g1_tight','g1_ptcone','g1_etcone','m_y1_is_isolated','g2_pt','g2_eta',\n",
    "                 'g2_phi','g2_E','g2_tight','g2_ptcone','g2_etcone','m_y2_is_isolated', 'p_mass', 'jet_n',\n",
    "                 'j1_pt','j1_eta','j1_phi','j1_E','j1_BTAG','j2_pt','j2_eta','j2_phi','j2_E','j2_BTAG']\n",
    "    \n",
    "    #Parse through each input file, process it, then write to the new files\n",
    "    for i in range(len(in_files)):\n",
    "\n",
    "        #Read CSV\n",
    "        df = pd.read_csv('./../CSVfiles/{}'.format(in_files[i]), names = col_names)\n",
    "        print('Starting {}'.format(in_files[i]))\n",
    "        \n",
    "        #Drop unused columns\n",
    "        df.drop(['m_y1_is_isolated', 'm_y2_is_isolated'], axis=1, inplace = True)\n",
    "            \n",
    "        #Calculate photon separation angle (eq: sqrt(eta1-eta2)^2+(phi1-phi2)^2))\n",
    "        del_r = np.sqrt((df['g1_eta']-df['g2_eta'])**2 + (df['g1_phi']-df['g2_phi'])**2)\n",
    "\n",
    "        #Add the new column to the start of the dataframe\n",
    "        df.insert(loc=0, column='photon_sep', value=del_r)\n",
    "            \n",
    "        #Isolate scale factor data, then remove this column to be left with useful data\n",
    "        if (in_files[i] == 'ggF_atlas.csv'):\n",
    "            sf_df = df['mcweight']/48.5 #Scaling by cross section\n",
    "        else:\n",
    "            sf_df = df['mcweight']\n",
    "        df.drop(['mcweight'], axis=1, inplace = True)\n",
    "        \n",
    "        #Generate a label column for signal/background\n",
    "        if (in_files[i] == 'ggF_atlas.csv'):\n",
    "            labels = np.ones(len(df))\n",
    "            df.insert(loc=0, column='label', value=labels)\n",
    "            #Move mass column to start of dataframe (mass is currently the 16th column)\n",
    "            cols = df.columns.tolist()\n",
    "            cols = [cols[0]] + [cols[16]]  + cols[1:16] + cols[17:]\n",
    "            df = df[cols]\n",
    "        elif (in_files[i] == 'yyjj_p1.csv'):\n",
    "            labels = np.zeros(len(df))\n",
    "            df.insert(loc=0, column='label', value=labels)\n",
    "            #Move mass column to start of dataframe (mass is currently the 16th column)\n",
    "            cols = df.columns.tolist()\n",
    "            cols = [cols[0]] + [cols[16]]  + cols[1:16] + cols[17:]\n",
    "            df = df[cols]\n",
    "        else:\n",
    "            #Move mass column to start of dataframe (mass is currently the 15th column)\n",
    "            #This is a different column as the raw data files have no labels\n",
    "            cols = df.columns.tolist()\n",
    "            cols =  [cols[15]] + [cols[0]] + cols[1:15] + cols[16:]\n",
    "            df = df[cols]\n",
    "            \n",
    "            \n",
    "        #Can remove rows of data set if at least 1 photon: is not tight, \n",
    "        #has ptCone/pt > 0.05, has etCone/eta > 0.065, mass outside 105 - 165 GeV range,\n",
    "        #or has pt/mass > 0.35 (0.25 for second photon)\n",
    "        pt1Ratio = df['g1_ptcone']/df['g1_pt']\n",
    "        pt2Ratio = df['g2_ptcone']/df['g2_pt']\n",
    "        et1Ratio = df['g1_etcone']/df['g1_pt']\n",
    "        et2Ratio = df['g2_etcone']/df['g2_pt']\n",
    "        pt_massRatio1 = df['g1_pt']/df['p_mass']\n",
    "        pt_massRatio2 = df['g2_pt']/df['p_mass']\n",
    "        \n",
    "        #These are for viewing the number of entries each condition removes\n",
    "        remove_tight = df[(df['g1_tight'] == 0) | (df['g2_tight'] == 0)].index\n",
    "        print('Tight condition: %.2f'%(len(remove_tight)/len(df)))\n",
    "        remove_ptratio = df[(pt1Ratio > 0.05) | (pt2Ratio > 0.05)].index\n",
    "        print('Ptratio condition: %.2f'%(len(remove_ptratio)/len(df)))\n",
    "        remove_etratio = df[(et1Ratio > 0.065) | (et2Ratio > 0.065)].index\n",
    "        print('Etratio condition: %.2f'%(len(remove_etratio)/len(df)))\n",
    "        remove_mass = df[(df['p_mass'] < 105000) | (df['p_mass'] > 165000)].index\n",
    "        print('Mass condition: %.2f'%(len(remove_mass)/len(df)))\n",
    "        remove_mass_ratio = df[(pt_massRatio1 < 0.35) & (pt_massRatio2 < 0.25)].index\n",
    "        print('Pt/Mass condition: %.2f'%(len(remove_mass_ratio)/len(df)))\n",
    "            \n",
    "        del remove_tight,remove_ptratio,remove_etratio,remove_mass\n",
    "        \n",
    "        #This list is now used to actually remove entries from the dataframe (but are the same conditions)\n",
    "        inds_to_remove = df[((df['g1_tight'] == 0) | (df['g2_tight'] == 0)) | ((pt1Ratio > 0.05) \n",
    "                        | (pt2Ratio > 0.05)) | ((et1Ratio > 0.065) | (et2Ratio > 0.065)) \n",
    "                        | ((df['p_mass'] < 105000) | (df['p_mass'] > 165000))\n",
    "                        | ((pt_massRatio1 < 0.35) & (pt_massRatio2 < 0.25))].index\n",
    "        print('Overall: %.2f'%(len(inds_to_remove)/len(df)))\n",
    "        \n",
    "        #Perform the removal of rows, but if we are processing one of the raw data files\n",
    "        #then we create a 'data_bkg' file which converts the excluded rows into a \n",
    "        #background dataset for optional use in training\n",
    "        if (in_files[i] == 'ggF_atlas.csv')|(in_files[i] == 'yyjj_p1.csv'):\n",
    "            df.drop(inds_to_remove, axis = 0, inplace = True)\n",
    "            #These columns are removed to stop the classifier from cheating\n",
    "            df.drop(['g1_ptcone','g2_ptcone','g1_etcone', 'g2_etcone', 'g1_tight', 'g2_tight'], axis = 1, inplace = True)\n",
    "        else:\n",
    "            bkg_df = df.iloc[inds_to_remove]\n",
    "            bkg_df.insert(loc=0, column='label', value=np.zeros(len(bkg_df))) #Adds labels\n",
    "            bkg_df['p_mass'] = bkg_df['p_mass']/1000\n",
    "            df.drop(['g1_ptcone','g2_ptcone','g1_etcone', 'g2_etcone', 'g1_tight', 'g2_tight'], axis = 1, inplace = True)\n",
    "            bkg_df.to_csv(\"./../CSVfiles/{}\".format(data_bkgs[i]), index = False, header = True) #Write the background data\n",
    "            df.drop(inds_to_remove, axis = 0, inplace = True) #Drop rows so that leftover is data passing conditions\n",
    "        \n",
    "            \n",
    "        #Make masses in GeV\n",
    "        df['p_mass'] = df['p_mass']/1000\n",
    "\n",
    "        #Write scale factor data\n",
    "        sf_df.to_csv(\"./../CSVfiles/{}\".format(sf_files[i]), index = False, header = True)\n",
    "        #Write all remaining data passing conditions\n",
    "        df.to_csv(\"./../CSVfiles/{}\".format(data_files[i]), index = False, header = True)\n",
    "                \n",
    "        print('Done {}'.format(in_files[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculates truth values upon a threshold cut\n",
    "def calc_Truths(y_pred, y_val, m, sf_val, threshold):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes rounded list of clasifier predictions (ints), corresponding true valiation\n",
    "    labels (ints), masses of each event (ints), scales factors for each event (ints), and threshold value (integer)\n",
    "    \n",
    "    Returns 2D lists of each classification where each element holds the mass (position 0)\n",
    "    and scale factor (position 1) for each respective event\n",
    "    \"\"\"\n",
    "\n",
    "    true_pos = []\n",
    "    false_pos = []\n",
    "    true_neg = []\n",
    "    false_neg = []\n",
    "\n",
    "    for i in range(len(y_val)):\n",
    "\n",
    "        if y_val[i] == 1 and y_pred[i] >= threshold:\n",
    "            true_pos.append([m[i], sf_val[i]])\n",
    "        elif y_val[i] == 0 and y_pred[i] >= threshold:\n",
    "            false_pos.append([m[i], sf_val[i]])\n",
    "        elif y_val[i] == 1 and y_pred[i] < threshold:\n",
    "            false_neg.append([m[i], sf_val[i]])\n",
    "        elif y_val[i] == 0 and y_pred[i] < threshold:\n",
    "            true_neg.append([m[i], sf_val[i]])\n",
    "\n",
    "    return true_pos, false_pos, true_neg, false_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sensitivity(true_pos, false_pos, sf_trim, SF_band):\n",
    "    \n",
    "    \"\"\"\n",
    "    First two inputs are 2D lists where each element holds the mass (position 0)\n",
    "    and scale factor (position 1) for an event, where the events are correct classifications \n",
    "    and misclassified background events respectively. The other inputs are the scale factor resulting\n",
    "    from the train test split, and the side band scale factor.\n",
    "    \n",
    "    Returns a value for the sensitivity\n",
    "    \"\"\"\n",
    "    \n",
    "    s, b, band_bkg = 0, 0, 0\n",
    "    #Add up scale factors of true positives which are between 121 - 129 GeV\n",
    "    for i in range(len(true_pos)): \n",
    "        if (true_pos[i][0] > 121) and (true_pos[i][0] < 129):\n",
    "            s += true_pos[i][1] \n",
    "    #Add up scale factors of false positives which are between 121 - 129 GeV\n",
    "    for i in range(len(false_pos)):\n",
    "        if (false_pos[i][0] > 121) and (false_pos[i][0] < 129):\n",
    "            b += false_pos[i][1]\n",
    "        #Add up scale factors of false positives which are outside 121 - 129 GeV\n",
    "        elif (false_pos[i][0] <= 121) or (false_pos[i][0] >= 129):\n",
    "            band_bkg += false_pos[i][1]\n",
    "    \n",
    "    #Multiply by train test split factor and (num expected events)/(num simulated events)\n",
    "    s = s*sf_trim[0]*(2.00419*(10**-3))\n",
    "    #Multiply by train test split factor and side band scale factor\n",
    "    b = b*sf_trim[1]*SF_band\n",
    "    n = (s+b)\n",
    "    #print('%.2f '%s,' %.2f '%b,' %.2f '%band_bkg)\n",
    "    sensitivity = np.sqrt(2*(n*np.log(n/b)+b-n))\n",
    "\n",
    "    return sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to turn a list into PyLorentz quantities\n",
    "def lorentzify(lst):\n",
    "\n",
    "    gamma_objects = []\n",
    "\n",
    "    #Separate each photon\n",
    "    each_gamma = np.split(lst, 2)\n",
    "\n",
    "    #Change each gamma into a PyLorentz object\n",
    "    for j in range(2):\n",
    "        gamma_objects.append(Momentum4.e_eta_phi_pt(each_gamma[j][3],each_gamma[j][1], each_gamma[j][2], each_gamma[j][0]))\n",
    "\n",
    "    return gamma_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use PyLorentz to calculate parent particle quantities\n",
    "def parent_Quantities(lst):\n",
    "\n",
    "    #Set memory placeholders for each list to avoid appends\n",
    "    inv_masses = np.zeros(len(lst))\n",
    "    trans_momenta = np.zeros(len(lst))\n",
    "    energies = np.zeros(len(lst))\n",
    "    etas = np.zeros(len(lst))\n",
    "    phis = np.zeros(len(lst))\n",
    "\n",
    "    for i in range(len(lst)):\n",
    "\n",
    "        #Turn list into PyLorentz objects\n",
    "        gammas = lorentzify(lst[i])\n",
    "        parent = gammas[0] + gammas[1]\n",
    "\n",
    "        #Calculate quantities\n",
    "        inv_masses[i] = parent.m\n",
    "        trans_momenta[i] = parent.p_t\n",
    "        energies[i] = parent.e\n",
    "        etas[i] = parent.eta\n",
    "        phis[i] = parent.phi\n",
    "    \n",
    "    return inv_masses, trans_momenta, energies, etas, phis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile (m, ys, labels=None, bins=np.linspace(100,160,60,endpoint=True), ax=None):\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \"\"\"\n",
    "    plt.rcParams.update({'font.size': 20})\n",
    "    # Check(s)\n",
    "    if isinstance(bins, int):\n",
    "        bins = np.linspace(m.min(), m.max(), bins + 1, endpoint=True)\n",
    "        pass\n",
    "\n",
    "    if not isinstance(ys, list):\n",
    "        ys = [ys]\n",
    "        pass\n",
    "\n",
    "    N = len(ys)\n",
    "    centres = bins[:-1] + 0.5 * np.diff(bins)\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [None for _ in range(N)]\n",
    "    elif isinstance(labels, str):\n",
    "        labels = [labels]\n",
    "        pass\n",
    "\n",
    "    assert len(labels) == N, \"[profile] Number of observables ({}) and associated labels ({}) do not match.\".format(N, len(labels))\n",
    "\n",
    "    # Local background efficiency\n",
    "    profiles = {ix: list() for ix in range(N)}\n",
    "    means_NN  = list()\n",
    "    means_ANN = list()\n",
    "    for down, up in zip(bins[:-1], bins[1:]):\n",
    "        msk = (m >= down) & (m < up)\n",
    "        for ix, y in enumerate(ys):\n",
    "            profiles[ix].append(y[msk].mean())\n",
    "            pass\n",
    "        pass\n",
    "\n",
    "    # Ensure axes exist\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize=(6,5))\n",
    "        pass\n",
    "\n",
    "    # Plot profile(s)\n",
    "    for ix in range(N):\n",
    "        ax.plot(centres, profiles[ix], '.-', label=labels[ix])\n",
    "        pass\n",
    "\n",
    "    # Decorations\n",
    "    ax.set_xlabel('Mass [GeV]')\n",
    "    ax.set_ylabel('Average Value')\n",
    "    ax.set_ylim((0,1))\n",
    "    ax.set_xlim(bins[0], bins[-1])\n",
    "    ax.legend()\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Data_Bkg(nrows):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function is for importing the background data which comes from \n",
    "    raw data events that did not pass our conditions\n",
    "    \"\"\"\n",
    "    \n",
    "    df1 = pd.read_csv('./../CSVfiles/data15_bkg.csv',nrows=int(nrows/4))\n",
    "    df2 = pd.read_csv('./../CSVfiles/data16_p1_bkg.csv',nrows=int(nrows/4))\n",
    "    df3 = pd.concat([df1,df2])\n",
    "    del df1,df2\n",
    "    df4 = pd.read_csv('./../CSVfiles/data16_p2_bkg.csv',nrows=int(nrows/4))\n",
    "    df5 = pd.read_csv('./../CSVfiles/data16_p3_bkg.csv',nrows=int(nrows/4))\n",
    "    df6 = pd.concat([df4,df5])\n",
    "    del df4,df5\n",
    "    df7 = pd.concat([df3,df6])\n",
    "    del df3,df6\n",
    "    \n",
    "    return df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Data_Sf(nrows):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function is for importing the background data which comes from \n",
    "    raw data events that did not pass our initial conditions\n",
    "    \"\"\"\n",
    "\n",
    "    df1 = pd.read_csv('./../CSVfiles/data15_bkg.csv',nrows=int(nrows/4))\n",
    "    df2 = pd.read_csv('./../CSVfiles/data16_p1_bkg.csv',nrows=int(nrows/4))\n",
    "    df3 = pd.concat([df1,df2])\n",
    "    del df1,df2\n",
    "    df4 = pd.read_csv('./../CSVfiles/data16_p2_bkg.csv',nrows=int(nrows/4))\n",
    "    df5 = pd.read_csv('./../CSVfiles/data16_p3_bkg.csv',nrows=int(nrows/4))\n",
    "    df6 = pd.concat([df4,df5])\n",
    "    del df4,df5\n",
    "    df7 = pd.concat([df3,df6])\n",
    "    del df3,df6\n",
    "    \n",
    "    return df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
